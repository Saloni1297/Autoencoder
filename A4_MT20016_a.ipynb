{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A4_MT20016_a.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Zw77fDztCKF3",
        "p_dY-2VcSd5K",
        "tVzCTrtZUlBm",
        "6pOwqSM5W2Rb"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw77fDztCKF3"
      },
      "source": [
        "# **Importing Library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyer3H-XCP3A"
      },
      "source": [
        "#importing libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy.linalg import eigh\n",
        "import seaborn as sn\n",
        "import random\n",
        "import math\n",
        "import gzip\n",
        "import struct\n",
        "from array import array\n",
        "from os.path  import join\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn import preprocessing\n",
        "from scipy.stats import multivariate_normal\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "from sklearn.model_selection import train_test_split\n",
        "from autograd import numpy as np\n",
        "from autograd import grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_dY-2VcSd5K"
      },
      "source": [
        "# **Generate 1000 multivariate gaussian points**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZB1-ginS8xQ"
      },
      "source": [
        "mean = [0,0,0]\n",
        "cov = [[1,0.8,0.8],[0.8,1,0.8],[0.8,0.8,1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBwqdACESkbF"
      },
      "source": [
        "np.random.seed(0)\n",
        "x = np.random.multivariate_normal(mean,cov,1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtLc_JCATfj5",
        "outputId": "2b468db0-0f3e-4e70-82c5-083f0cc86913"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVzCTrtZUlBm"
      },
      "source": [
        "# **Normalize the data so that value in each dimension lies in [0,1] and a sigmoid can be used in the output layer.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egwye75_Utge"
      },
      "source": [
        "\n",
        "normalized_x = 1/(1 + np.exp(-x))   #sigmoid normalization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rwdoz-mDU_sw",
        "outputId": "9d1e06c5-be3f-4154-9d92-ac71915b18fa"
      },
      "source": [
        "normalized_x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.21672429, 0.12482878, 0.15519875],\n",
              "       [0.07995126, 0.07597702, 0.21129141],\n",
              "       [0.28451506, 0.30623601, 0.28628367],\n",
              "       ...,\n",
              "       [0.78109511, 0.7381197 , 0.79107501],\n",
              "       [0.34767081, 0.31222299, 0.62837217],\n",
              "       [0.53735232, 0.64043782, 0.45274214]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pOwqSM5W2Rb"
      },
      "source": [
        "# **Take 800 samples for training and call the remaining samples test set.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQbUJffWXLps"
      },
      "source": [
        "x_train = normalized_x[:800]\n",
        "x_test = normalized_x[800:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtq8HQGko5kb",
        "outputId": "b1b258c3-f80a-4c37-fd7f-a7918573eaa7"
      },
      "source": [
        "len(x_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZLfrKGnXz-f"
      },
      "source": [
        "# **Implement an autoencoder with one input layer, one hidden layer, and one output layer. Train the model using data generated in part 1.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4zhHKL-gQ2F"
      },
      "source": [
        "def print_graph(x, y, num):\n",
        "  lst = []\n",
        "  for i in range(num):\n",
        "    lst.append(i)\n",
        "  plt.plot(lst,x,'red',label=\"Train Loss\")\n",
        "  plt.plot(lst,y,'blue',label=\"Test Loss\")\n",
        "  plt.xlabel(\"Number of iterations/epochs\")\n",
        "  plt.ylabel(\"Losses\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDLuwEgRQVPX"
      },
      "source": [
        "def print_loss(x,y,num):\n",
        "  for i in range(num):\n",
        "    print(f\"train loss : {x[i]} , test_loss : {y[i]}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YQCOTXerAnw"
      },
      "source": [
        "#sigmoid \n",
        "def sigmoid(x):\n",
        "  z = 1/(1 + np.exp(-x))\n",
        "  return z\n",
        "\n",
        "#derivative of sigmoid\n",
        "def sigmoid_dev(x):\n",
        "  z = sigmoid(x)\n",
        "  return (z*(1-z))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFRsBOJfMUTj"
      },
      "source": [
        "def update_weights(W,w,b,c,i,n):\n",
        "  a1 = np.dot(i,W)+b  #1x2\n",
        "  #print(\"a1\",a1.shape)\n",
        "  h = sigmoid(a1) #1x2\n",
        "  #print(\"h\",h.shape)\n",
        "  a2 = np.dot(h,w)+c #1x3\n",
        "  #print(\"a2\",a2.shape)\n",
        "  x = sigmoid(a2)  #1x3\n",
        "  #print(\"x\",x.shape)\n",
        "  diff = (1-sigmoid(a2))\n",
        "  t = np.dot(sigmoid(a2),diff.T)\n",
        "  a2 = np.reshape(a2,(-1,3))\n",
        "  e_a2 = t*(i-x)*(-2)   #1x3\n",
        "  #print(\"e_a2\",e_a2.shape)\n",
        "  h = np.reshape(h, (-1, 2))\n",
        "  e_a2 = np.reshape(e_a2,(-1,3)) \n",
        "  ww = w-n*np.dot(h.T,e_a2)   #2x3\n",
        "  #print(\"ww\",ww.shape)\n",
        "  cc = c-n*e_a2 #1x3\n",
        "  #print(\"c\",cc.shape)\n",
        "  h = np.reshape(h,(2,-1))\n",
        "  e_h = np.dot(w,e_a2.T)  #2x1\n",
        "  #print(\"e_h\",e_h.shape)\n",
        "  diff1 = (1-sigmoid(a1))\n",
        "  t1 = np.dot(sigmoid(a1),diff1.T)\n",
        "  a1 = np.reshape(a1,(-1,2))\n",
        "  e_a1 = t1*e_h.T   #1x2\n",
        "  #print(\"e_a1\",e_a1.shape)\n",
        "  i = np.reshape(i,(-1,3))\n",
        "  WW = W-n*(i.T*e_a1)  #3x2\n",
        "  #print(\"WW\",WW.shape)\n",
        "  bb = b - e_a1  #1X2\n",
        "  #print(\"bb\",bb.shape)\n",
        "  return WW,ww,bb,cc\n",
        "\n",
        "\n",
        "# i = x_train[0]\n",
        "# W,w,b,c = update_weights(W,w,b,c,i,n)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQhElgKxuvMY"
      },
      "source": [
        "def forward_backward_pass(W,w,b,c,n,i):\n",
        "  a1 = np.dot(i,W)+b  #1x2\n",
        "  h = sigmoid(a1) #1x2\n",
        "  a2 = np.dot(h,w)+c #1x3\n",
        "  x = sigmoid(a2)  #1x3\n",
        "  diff = (-2)*(i-x)\n",
        "  t = sigmoid_dev(a2)\n",
        "  e_a2 = t*diff #1x3\n",
        "  h = np.reshape(h, (-1, 2))\n",
        "  e_a2 = np.reshape(e_a2,(-1,3)) \n",
        "  ww = w-n*np.dot(h.T,e_a2)   #2x3\n",
        "  cc = c-n*e_a2 #1x3\n",
        "  h = np.reshape(h,(2,-1))\n",
        "  e_h = np.dot(w,e_a2.T)  #2x1\n",
        "  t1 = sigmoid_dev(a1)\n",
        "  a1 = np.reshape(a1,(-1,2))\n",
        "  e_a1 = t1*e_h.T   #1x2\n",
        "  i = np.reshape(i,(-1,3))\n",
        "  WW = W-n*(i.T*e_a1)  #3x2\n",
        "  bb = b-n*e_a1  #1X2\n",
        "  return WW,ww,bb,cc,x\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnT1_PYlvKT_"
      },
      "source": [
        "def forward_pass(W,w,b,c,i):\n",
        "  a1 = np.dot(i,W)+b  #1x2\n",
        "  h = sigmoid(a1) #1x2\n",
        "  a2 = np.dot(h,w)+c #1x3\n",
        "  x = sigmoid(a2)  #1x3\n",
        "  return a1,h,a2,x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8V8lbAML_Kh"
      },
      "source": [
        "def cal_test_train_loss(W,w,b,c,x_train,x_test,epochs,n):\n",
        "  train = []\n",
        "  test = []\n",
        "  for j in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    test_loss = 0.0\n",
        "    for i in x_train:\n",
        "      i = np.reshape(i,(-1,3))\n",
        "      W,w,b,c,x = forward_backward_pass(W,w,b,c,n,i)\n",
        "      train_loss += np.sum((i-x)**2)\n",
        "    train.append(train_loss)\n",
        "\n",
        "   \n",
        "    for k in x_test:\n",
        "      k = np.reshape(k,(-1,3))\n",
        "      a1,h,a2,x = forward_pass(W,w,b,c,k)\n",
        "      test_loss += np.sum((k-x)**2)\n",
        "    test.append(test_loss)\n",
        "\n",
        "  return train,test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IhLmJLUIr4Vo",
        "outputId": "18e1e3ef-5767-420c-97fc-198623695202"
      },
      "source": [
        "#intial values\n",
        "b = [0.01, 0.01]\n",
        "b = np.array(b)\n",
        "W = [[0.01,0.02],[0.01,0.02],[0.01,0.02]]\n",
        "W = np.array(W)\n",
        "w = [[0.01,0.02,0.03],[0.01,0.02,0.03]]\n",
        "w = np.array(w)\n",
        "c = [0.02,0.02,0.02]\n",
        "c = np.array(c)\n",
        "n = 0.1\n",
        "epochs = 200\n",
        "diff_train,diff_test = cal_test_train_loss(W,w,b,c,x_train,x_test,epochs,n)\n",
        "print_loss(diff_train,diff_test,epochs)\n",
        "print_graph(diff_train,diff_test,epochs)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss : 100.59466248540429 , test_loss : 22.71415480202921\n",
            "train loss : 100.52536459217379 , test_loss : 22.692330572198063\n",
            "train loss : 100.29279664996415 , test_loss : 22.5748927662136\n",
            "train loss : 99.04562742780318 , test_loss : 22.01132839064522\n",
            "train loss : 94.14569263006123 , test_loss : 20.187099796321792\n",
            "train loss : 80.98573672219159 , test_loss : 15.891535430230356\n",
            "train loss : 56.28835968449856 , test_loss : 9.659844880851074\n",
            "train loss : 32.49919078514522 , test_loss : 5.676239561089266\n",
            "train loss : 21.00497533274131 , test_loss : 4.236234877463098\n",
            "train loss : 17.171133469820685 , test_loss : 3.819987258129053\n",
            "train loss : 16.019727427648245 , test_loss : 3.7080223200463034\n",
            "train loss : 15.6654553371574 , test_loss : 3.6772960330162645\n",
            "train loss : 15.540997519007785 , test_loss : 3.6666078530149684\n",
            "train loss : 15.484214692532813 , test_loss : 3.660358947397922\n",
            "train loss : 15.448847229455101 , test_loss : 3.654977061247841\n",
            "train loss : 15.421304932600508 , test_loss : 3.649814277905477\n",
            "train loss : 15.397285063946045 , test_loss : 3.6448251072218145\n",
            "train loss : 15.375229172625717 , test_loss : 3.6400365797308747\n",
            "train loss : 15.354465661503205 , test_loss : 3.63545715840746\n",
            "train loss : 15.334654869854127 , test_loss : 3.6310766973629063\n",
            "train loss : 15.31560425818263 , test_loss : 3.6268769932568508\n",
            "train loss : 15.297196131558614 , test_loss : 3.622838668868248\n",
            "train loss : 15.279354320814896 , test_loss : 3.6189442208196594\n",
            "train loss : 15.262026850131821 , test_loss : 3.6151788952224773\n",
            "train loss : 15.245176258381033 , test_loss : 3.6115305948122955\n",
            "train loss : 15.228773981970404 , test_loss : 3.607989453621545\n",
            "train loss : 15.212797020334065 , test_loss : 3.6045473639858026\n",
            "train loss : 15.19722592506809 , test_loss : 3.6011975621516203\n",
            "train loss : 15.182043571333784 , test_loss : 3.5979342985150566\n",
            "train loss : 15.167234397889015 , test_loss : 3.5947525870730734\n",
            "train loss : 15.152783930930354 , test_loss : 3.5916480192327676\n",
            "train loss : 15.138678481392128 , test_loss : 3.588616626595535\n",
            "train loss : 15.12490494905618 , test_loss : 3.5856547797851053\n",
            "train loss : 15.111450692808765 , test_loss : 3.582759113362222\n",
            "train loss : 15.09830344200261 , test_loss : 3.579926469486847\n",
            "train loss : 15.085451233372366 , test_loss : 3.5771538550370026\n",
            "train loss : 15.072882363770232 , test_loss : 3.5744384084074983\n",
            "train loss : 15.060585352585449 , test_loss : 3.5717773732980778\n",
            "train loss : 15.048548909950892 , test_loss : 3.5691680775687598\n",
            "train loss : 15.036761908243335 , test_loss : 3.5666079157796187\n",
            "train loss : 15.025213355263158 , test_loss : 3.564094334411212\n",
            "train loss : 15.01389236803732 , test_loss : 3.561624819028619\n",
            "train loss : 15.002788146537014 , test_loss : 3.5591968828408564\n",
            "train loss : 14.991889946820955 , test_loss : 3.5568080562416644\n",
            "train loss : 14.981187053251476 , test_loss : 3.554455877013829\n",
            "train loss : 14.970668749510704 , test_loss : 3.5521378809481985\n",
            "train loss : 14.96032428819348 , test_loss : 3.5498515926783294\n",
            "train loss : 14.950142858775878 , test_loss : 3.547594516567406\n",
            "train loss : 14.940113553768631 , test_loss : 3.5453641275096293\n",
            "train loss : 14.930225332862543 , test_loss : 3.543157861526057\n",
            "train loss : 14.920466984864548 , test_loss : 3.540973106046946\n",
            "train loss : 14.910827087207513 , test_loss : 3.53880718978005\n",
            "train loss : 14.901293962797892 , test_loss : 3.5366573720681416\n",
            "train loss : 14.891855633940256 , test_loss : 3.5345208316396692\n",
            "train loss : 14.882499773048995 , test_loss : 3.5323946546545213\n",
            "train loss : 14.873213649823134 , test_loss : 3.530275821942204\n",
            "train loss : 14.863984074520722 , test_loss : 3.5281611953229755\n",
            "train loss : 14.854797336923001 , test_loss : 3.526047502893069\n",
            "train loss : 14.84563914052438 , test_loss : 3.523931323143207\n",
            "train loss : 14.836494531421828 , test_loss : 3.5218090677650706\n",
            "train loss : 14.82734782130429 , test_loss : 3.5196769629826976\n",
            "train loss : 14.818182503856379 , test_loss : 3.5175310292246906\n",
            "train loss : 14.808981163791978 , test_loss : 3.5153670589283643\n",
            "train loss : 14.799725377614834 , test_loss : 3.5131805922374797\n",
            "train loss : 14.790395605067014 , test_loss : 3.5109668903211415\n",
            "train loss : 14.780971070065494 , test_loss : 3.5087209060010904\n",
            "train loss : 14.771429629738986 , test_loss : 3.5064372513279287\n",
            "train loss : 14.761747629956563 , test_loss : 3.5041101616922927\n",
            "train loss : 14.75189974548283 , test_loss : 3.5017334559935427\n",
            "train loss : 14.741858802591896 , test_loss : 3.499300492314779\n",
            "train loss : 14.731595581620494 , test_loss : 3.4968041184677188\n",
            "train loss : 14.721078596528091 , test_loss : 3.494236616671491\n",
            "train loss : 14.710273848051656 , test_loss : 3.491589641515112\n",
            "train loss : 14.699144546480431 , test_loss : 3.4888541502204795\n",
            "train loss : 14.687650799423716 , test_loss : 3.486020324069772\n",
            "train loss : 14.675749259182963 , test_loss : 3.483077479685073\n",
            "train loss : 14.663392723455747 , test_loss : 3.4800139686451796\n",
            "train loss : 14.650529682074747 , test_loss : 3.4768170636928626\n",
            "train loss : 14.637103801297847 , test_loss : 3.4734728295206154\n",
            "train loss : 14.62305333579827 , test_loss : 3.4699659758221753\n",
            "train loss : 14.608310456930884 , test_loss : 3.4662796899569757\n",
            "train loss : 14.592800484053011 , test_loss : 3.4623954461936592\n",
            "train loss : 14.57644100363515 , test_loss : 3.4582927880764007\n",
            "train loss : 14.559140858595022 , test_loss : 3.453949079995534\n",
            "train loss : 14.540798987726143 , test_loss : 3.449339223549154\n",
            "train loss : 14.521303092281084 , test_loss : 3.444435333766656\n",
            "train loss : 14.500528103761335 , test_loss : 3.439206369752089\n",
            "train loss : 14.478334423845158 , test_loss : 3.4336177138316435\n",
            "train loss : 14.454565904326186 , test_loss : 3.427630692914929\n",
            "train loss : 14.42904753220743 , test_loss : 3.421202035594083\n",
            "train loss : 14.401582783142041 , test_loss : 3.4142832586440472\n",
            "train loss : 14.371950605911266 , test_loss : 3.40681997725044\n",
            "train loss : 14.339902002607959 , test_loss : 3.3987511347669632\n",
            "train loss : 14.305156175170376 , test_loss : 3.3900081505029505\n",
            "train loss : 14.267396221101063 , test_loss : 3.3805139885395543\n",
            "train loss : 14.22626438277243 , test_loss : 3.370182157664001\n",
            "train loss : 14.181356890112157 , test_loss : 3.35891566326392\n",
            "train loss : 14.132218491774811 , test_loss : 3.346605947838838\n",
            "train loss : 14.078336853275294 , test_loss : 3.3331318794256486\n",
            "train loss : 14.019137122407315 , test_loss : 3.3183588787898133\n",
            "train loss : 13.953977135184207 , test_loss : 3.3021383189436113\n",
            "train loss : 13.882143973278305 , test_loss : 3.284307386349684\n",
            "train loss : 13.80285289869791 , test_loss : 3.264689662739729\n",
            "train loss : 13.715250088060905 , test_loss : 3.243096767550505\n",
            "train loss : 13.618421053939658 , test_loss : 3.2193314854984805\n",
            "train loss : 13.511407124853191 , test_loss : 3.1931928738327677\n",
            "train loss : 13.393232746743914 , test_loss : 3.164483866459517\n",
            "train loss : 13.262946461877968 , test_loss : 3.1330218155542884\n",
            "train loss : 13.11967789460869 , test_loss : 3.0986521660255355\n",
            "train loss : 12.962711507918224 , test_loss : 3.0612649724837078\n",
            "train loss : 12.791574886794212 , test_loss : 3.0208132076411824\n",
            "train loss : 12.606134737916543 , test_loss : 2.977330846013384\n",
            "train loss : 12.406688267863869 , test_loss : 2.9309477892362246\n",
            "train loss : 12.194032827078864 , test_loss : 2.881898279782374\n",
            "train loss : 11.96949542917102 , test_loss : 2.830520036636479\n",
            "train loss : 11.734908641532726 , test_loss : 2.777243162683952\n",
            "train loss : 11.49253087430895 , test_loss : 2.722570475108972\n",
            "train loss : 11.244923793805924 , test_loss : 2.6670531560097284\n",
            "train loss : 10.99481076899369 , test_loss : 2.6112662831549662\n",
            "train loss : 10.744942029326765 , test_loss : 2.5557875035095763\n",
            "train loss : 10.497983973181935 , test_loss : 2.5011797347107594\n",
            "train loss : 10.256437242477533 , test_loss : 2.447976821329599\n",
            "train loss : 10.022578084040001 , test_loss : 2.3966705199564737\n",
            "train loss : 9.798414215537441 , test_loss : 2.3476979362504755\n",
            "train loss : 9.585649005411012 , test_loss : 2.3014297519462086\n",
            "train loss : 9.385652572303124 , test_loss : 2.2581604416813628\n",
            "train loss : 9.199442073174998 , test_loss : 2.2181018441345897\n",
            "train loss : 9.027674593003251 , test_loss : 2.1813810318197593\n",
            "train loss : 8.870655012710657 , test_loss : 2.1480427329934044\n",
            "train loss : 8.728359107358319 , test_loss : 2.118055887551823\n",
            "train loss : 8.60046995369895 , test_loss : 2.0913234361648674\n",
            "train loss : 8.486424147055793 , test_loss : 2.0676942002015455\n",
            "train loss : 8.385463573371961 , test_loss : 2.046975685067313\n",
            "train loss : 8.296688498139295 , test_loss : 2.0289467722907766\n",
            "train loss : 8.219108317004004 , test_loss : 2.013369489985679\n",
            "train loss : 8.15168721813778 , test_loss : 1.999999309283921\n",
            "train loss : 8.093383012316965 , test_loss : 1.9885936627544714\n",
            "train loss : 8.043178327330132 , test_loss : 1.9789185923475814\n",
            "train loss : 8.000104137654949 , test_loss : 1.9707535958432838\n",
            "train loss : 7.96325616509485 , test_loss : 1.9638948498911624\n",
            "train loss : 7.931805041708941 , test_loss : 1.9581570495564873\n",
            "train loss : 7.905001300796804 , test_loss : 1.9533741278275443\n",
            "train loss : 7.882176295803475 , test_loss : 1.949399114063926\n",
            "train loss : 7.862740083938234 , test_loss : 1.9461033676227297\n",
            "train loss : 7.84617718972308 , test_loss : 1.9433753900875763\n",
            "train loss : 7.832041014515975 , test_loss : 1.9411193829002242\n",
            "train loss : 7.819947503650926 , test_loss : 1.9392536810937495\n",
            "train loss : 7.809568537750963 , test_loss : 1.9377091609339845\n",
            "train loss : 7.8006253871931825 , test_loss : 1.9364276909838665\n",
            "train loss : 7.79288246213976 , test_loss : 1.9353606728802022\n",
            "train loss : 7.786141505320678 , test_loss : 1.9344676998169046\n",
            "train loss : 7.780236309269947 , test_loss : 1.933715346874665\n",
            "train loss : 7.7750279913832 , test_loss : 1.933076097271133\n",
            "train loss : 7.770400826037387 , test_loss : 1.9325274016326885\n",
            "train loss : 7.766258610196732 , test_loss : 1.9320508628489763\n",
            "train loss : 7.762521524802208 , test_loss : 1.9316315363796408\n",
            "train loss : 7.759123446568633 , test_loss : 1.9312573345471413\n",
            "train loss : 7.756009661758422 , test_loss : 1.9309185229717793\n",
            "train loss : 7.753134933619171 , test_loss : 1.9306072975729376\n",
            "train loss : 7.750461877351625 , test_loss : 1.9303174312379183\n",
            "train loss : 7.747959599892579 , test_loss : 1.9300439801718419\n",
            "train loss : 7.745602565859803 , test_loss : 1.9297830409646854\n",
            "train loss : 7.74336965529704 , test_loss : 1.9295315504592299\n",
            "train loss : 7.741243383103299 , test_loss : 1.9292871215201428\n",
            "train loss : 7.739209254053181 , test_loss : 1.92904790875527\n",
            "train loss : 7.737255231017584 , test_loss : 1.9288124991065827\n",
            "train loss : 7.735371297323539 , test_loss : 1.928579823001869\n",
            "train loss : 7.73354909713772 , test_loss : 1.9283490824385243\n",
            "train loss : 7.731781640327779 , test_loss : 1.9281196929612734\n",
            "train loss : 7.7300630604725225 , test_loss : 1.9278912370030659\n",
            "train loss : 7.728388416587131 , test_loss : 1.9276634264904495\n",
            "train loss : 7.726753530738295 , test_loss : 1.927436072980125\n",
            "train loss : 7.725154855077937 , test_loss : 1.9272090639002981\n",
            "train loss : 7.7235893629614 , test_loss : 1.926982343726885\n",
            "train loss : 7.722054459761781 , test_loss : 1.9267558991378986\n",
            "train loss : 7.720547909779989 , test_loss : 1.9265297473658527\n",
            "train loss : 7.719067776300239 , test_loss : 1.9263039271137523\n",
            "train loss : 7.717612372378549 , test_loss : 1.9260784915198796\n",
            "train loss : 7.716180220393006 , test_loss : 1.9258535027547798\n",
            "train loss : 7.714770018747566 , test_loss : 1.9256290279140924\n",
            "train loss : 7.713380614417999 , test_loss : 1.9254051359361533\n",
            "train loss : 7.71201098027155 , test_loss : 1.925181895326655\n",
            "train loss : 7.710660196289691 , test_loss : 1.9249593725156522\n",
            "train loss : 7.709327433985519 , test_loss : 1.9247376307072979\n",
            "train loss : 7.708011943438397 , test_loss : 1.9245167291108776\n",
            "train loss : 7.706713042475761 , test_loss : 1.9242967224645118\n",
            "train loss : 7.705430107619346 , test_loss : 1.9240776607813173\n",
            "train loss : 7.7041625664831646 , test_loss : 1.923859589262476\n",
            "train loss : 7.702909891369113 , test_loss : 1.923642548333573\n",
            "train loss : 7.7016715938516445 , test_loss : 1.9234265737699752\n",
            "train loss : 7.700447220182071 , test_loss : 1.923211696884616\n",
            "train loss : 7.699236347373017 , test_loss : 1.9229979447575714\n",
            "train loss : 7.698038579849311 , test_loss : 1.9227853404915973\n",
            "train loss : 7.696853546571505 , test_loss : 1.9225739034815716\n",
            "train loss : 7.695680898555252 , test_loss : 1.9223636496888128\n",
            "train loss : 7.69452030672309 , test_loss : 1.922154591913584\n",
            "train loss : 7.693371460036401 , test_loss : 1.9219467400609465\n",
            "train loss : 7.692234063864072 , test_loss : 1.9217401013965316\n",
            "train loss : 7.691107838552121 , test_loss : 1.9215346807899787\n",
            "train loss : 7.689992518164406 , test_loss : 1.9213304809445646\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZU-cNA2qacx"
      },
      "source": [
        "# **Perform backpropagation using autograd and compare the results with previous part.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjrhoLk-gp6w"
      },
      "source": [
        "def forward_pass_loss(i,W,w,b,c):\n",
        "  a1 = np.dot(i,W)+b  #1x2\n",
        "  h = sigmoid(a1) #1x2\n",
        "  a2 = np.dot(h,w)+c #1x3\n",
        "  x = sigmoid(a2)  #1x3\n",
        "  l = np.sum((i-x)**2)\n",
        "  return l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfumLGt0sEwv"
      },
      "source": [
        "def update_weights(WW,ww,bb,cc,W_update,ww_update,b_update,c_update,n):\n",
        "    w = ww - n * ww_update\n",
        "    b = bb - n * b_update\n",
        "    W = WW - n * W_update\n",
        "    c = cc - n * c_update\n",
        "    return W,w,b,c\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAhUEBnET3EE"
      },
      "source": [
        "def cal_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqZiM2_JUe-7"
      },
      "source": [
        "def cal_grad_loss(WW,ww,bb,cc,x_train,x_test,epochs,n):\n",
        "  gradient = grad(forward_pass_loss,(1,2,3,4))\n",
        "  train = []\n",
        "  test = []\n",
        "  for j in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    test_loss = 0.0\n",
        "    for i in x_train:\n",
        "      train_loss += forward_pass_loss(i,WW,ww,bb,cc) \n",
        "      W_update,ww_update,b_update,c_update = gradient(i,WW,ww,bb,cc)\n",
        "      WW,ww,bb,cc = update_weights(WW,ww,bb,cc,W_update,ww_update,b_update,c_update,n)\n",
        "    for i in x_test:\n",
        "      test_loss += forward_pass_loss(i,WW,ww,bb,cc)\n",
        "    train.append(train_loss)\n",
        "    test.append(test_loss)\n",
        "  return train,test\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WdN8uNvRdfxG",
        "outputId": "b9301602-f0d5-46ae-ed6f-b7446c77b218"
      },
      "source": [
        "#intial values\n",
        "b = [0.01, 0.01]\n",
        "b = np.array(b)\n",
        "W = [[0.01,0.02],[0.01,0.02],[0.01,0.02]]\n",
        "W = np.array(W)\n",
        "w = [[0.01,0.02,0.03],[0.01,0.02,0.03]]\n",
        "w = np.array(w)\n",
        "c = [0.02,0.02,0.02]\n",
        "c = np.array(c)\n",
        "n = 0.1\n",
        "epochs = 200\n",
        "diff_train,diff_test = cal_grad_loss(W,w,b,c,x_train,x_test,epochs,n)\n",
        "print_loss(diff_train,diff_test,epochs)\n",
        "print_graph(diff_train,diff_test,epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss : 98.36155999173266 , test_loss : 26.419468705042412\n",
            "train loss : 98.31788352847082 , test_loss : 26.416761906332756\n",
            "train loss : 98.12664891800331 , test_loss : 26.331201948229605\n",
            "train loss : 97.00860256424312 , test_loss : 25.738255111783108\n",
            "train loss : 92.1705566760851 , test_loss : 23.55566747850442\n",
            "train loss : 79.50274767379985 , test_loss : 18.797543338686154\n",
            "train loss : 57.396531965510235 , test_loss : 12.155653848829713\n",
            "train loss : 35.603044650511826 , test_loss : 7.476329940382669\n",
            "train loss : 23.73540012806695 , test_loss : 5.4092351444351285\n",
            "train loss : 19.08661995717321 , test_loss : 4.62184179462628\n",
            "train loss : 17.429147035869423 , test_loss : 4.315756125355376\n",
            "train loss : 16.818081718062025 , test_loss : 4.183786938162689\n",
            "train loss : 16.56462323492889 , test_loss : 4.118019257826345\n",
            "train loss : 16.439513396468268 , test_loss : 4.080025082625339\n",
            "train loss : 16.36546627660906 , test_loss : 4.05513638300767\n",
            "train loss : 16.31457091240939 , test_loss : 4.037134605582335\n",
            "train loss : 16.27552360257448 , test_loss : 4.023074194887082\n",
            "train loss : 16.24312852411841 , test_loss : 4.01142549126247\n",
            "train loss : 16.214738913016497 , test_loss : 4.001338067672223\n",
            "train loss : 16.18891042438355 , test_loss : 3.9923141569904717\n",
            "train loss : 16.164819497247887 , test_loss : 3.984049899137169\n",
            "train loss : 16.14198211190766 , test_loss : 3.9763528664169367\n",
            "train loss : 16.120106822700954 , test_loss : 3.969097078742719\n",
            "train loss : 16.099014063602155 , test_loss : 3.962197483932659\n",
            "train loss : 16.078590379528766 , test_loss : 3.955594976576227\n",
            "train loss : 16.058761870375545 , test_loss : 3.9492473301217577\n",
            "train loss : 16.039478524239318 , test_loss : 3.943123555711992\n",
            "train loss : 16.02070486028888 , test_loss : 3.93720030601616\n",
            "train loss : 16.00241429017288 , test_loss : 3.93145953214029\n",
            "train loss : 15.984585699691756 , test_loss : 3.9258869266548975\n",
            "train loss : 15.967201369248846 , test_loss : 3.920470870226628\n",
            "train loss : 15.950245707250517 , test_loss : 3.9152017069744804\n",
            "train loss : 15.933704479324058 , test_loss : 3.910071238074982\n",
            "train loss : 15.917564340458886 , test_loss : 3.905072362546672\n",
            "train loss : 15.901812552000084 , test_loss : 3.900198818734573\n",
            "train loss : 15.88643681091063 , test_loss : 3.8954449956309003\n",
            "train loss : 15.871425146574675 , test_loss : 3.890805793242133\n",
            "train loss : 15.856765857564094 , test_loss : 3.8862765178050447\n",
            "train loss : 15.842447471385373 , test_loss : 3.8818528020269576\n",
            "train loss : 15.828458716781157 , test_loss : 3.8775305434624356\n",
            "train loss : 15.814788502212837 , test_loss : 3.8733058561354974\n",
            "train loss : 15.801425896649686 , test_loss : 3.869175031889601\n",
            "train loss : 15.788360110316747 , test_loss : 3.8651345089026674\n",
            "train loss : 15.775580473977636 , test_loss : 3.861180845475114\n",
            "train loss : 15.763076415872154 , test_loss : 3.8573106976749134\n",
            "train loss : 15.750837435738834 , test_loss : 3.8535207997637384\n",
            "train loss : 15.738853075514852 , test_loss : 3.8498079465731236\n",
            "train loss : 15.72711288637627 , test_loss : 3.8461689771764935\n",
            "train loss : 15.715606391794996 , test_loss : 3.842600759330594\n",
            "train loss : 15.704323046266113 , test_loss : 3.839100174251745\n",
            "train loss : 15.693252189310638 , test_loss : 3.83566410135737\n",
            "train loss : 15.682382994295775 , test_loss : 3.8322894026478562\n",
            "train loss : 15.671704411536536 , test_loss : 3.828972906432728\n",
            "train loss : 15.661205105053298 , test_loss : 3.825711390121412\n",
            "train loss : 15.650873382260214 , test_loss : 3.8225015618046587\n",
            "train loss : 15.640697115744912 , test_loss : 3.8193400403496662\n",
            "train loss : 15.63066365617464 , test_loss : 3.8162233337208598\n",
            "train loss : 15.620759735219046 , test_loss : 3.8131478152197884\n",
            "train loss : 15.610971357217222 , test_loss : 3.8101096973115727\n",
            "train loss : 15.601283678132305 , test_loss : 3.8071050026719475\n",
            "train loss : 15.591680870121847 , test_loss : 3.804129532047412\n",
            "train loss : 15.582145969808765 , test_loss : 3.8011788284706975\n",
            "train loss : 15.572660708053586 , test_loss : 3.7982481373135526\n",
            "train loss : 15.563205318699596 , test_loss : 3.795332361587473\n",
            "train loss : 15.553758323379396 , test_loss : 3.7924260118185003\n",
            "train loss : 15.544296289023494 , test_loss : 3.7895231497225668\n",
            "train loss : 15.534793554186653 , test_loss : 3.786617324790385\n",
            "train loss : 15.525221919690946 , test_loss : 3.7837015027523044\n",
            "train loss : 15.515550298358482 , test_loss : 3.7807679847302693\n",
            "train loss : 15.505744317750507 , test_loss : 3.777808315691116\n",
            "train loss : 15.495765868815862 , test_loss : 3.774813180587954\n",
            "train loss : 15.485572592157895 , test_loss : 3.7717722863078618\n",
            "train loss : 15.47511729221022 , test_loss : 3.768674227227246\n",
            "train loss : 15.464347267940298 , test_loss : 3.765506331803191\n",
            "train loss : 15.453203546721774 , test_loss : 3.7622544871905066\n",
            "train loss : 15.441620005690451 , test_loss : 3.7589029383607686\n",
            "train loss : 15.429522362166162 , test_loss : 3.7554340576011604\n",
            "train loss : 15.416827011537768 , test_loss : 3.7518280795793832\n",
            "train loss : 15.403439687323488 , test_loss : 3.748062796368954\n",
            "train loss : 15.389253913903541 , test_loss : 3.7441132059364413\n",
            "train loss : 15.374149217684653 , test_loss : 3.7399511066060582\n",
            "train loss : 15.357989057252665 , test_loss : 3.7355446289613643\n",
            "train loss : 15.340618427566044 , test_loss : 3.7308576955670283\n",
            "train loss : 15.321861087751271 , test_loss : 3.72584939788417\n",
            "train loss : 15.30151635714929 , test_loss : 3.7204732789578556\n",
            "train loss : 15.279355420864237 , test_loss : 3.7146765101122208\n",
            "train loss : 15.255117085678375 , test_loss : 3.708398950361885\n",
            "train loss : 15.228502932110045 , test_loss : 3.701572079084272\n",
            "train loss : 15.199171822024612 , test_loss : 3.694117796487606\n",
            "train loss : 15.16673374847569 , test_loss : 3.68594709366427\n",
            "train loss : 15.130743062200187 , test_loss : 3.6769586060345265\n",
            "train loss : 15.090691186477272 , test_loss : 3.667037082672093\n",
            "train loss : 15.045999050178372 , test_loss : 3.6560518316173543\n",
            "train loss : 14.996009640485585 , test_loss : 3.643855240160143\n",
            "train loss : 14.939981313761304 , test_loss : 3.6302815210044566\n",
            "train loss : 14.877082811528503 , test_loss : 3.6151459003044786\n",
            "train loss : 14.806391300452725 , test_loss : 3.598244538347223\n",
            "train loss : 14.726895155939982 , test_loss : 3.579355548759369\n",
            "train loss : 14.63750356225344 , test_loss : 3.5582415398316756\n",
            "train loss : 14.537065178948305 , test_loss : 3.5346541151865796\n",
            "train loss : 14.424397947726211 , test_loss : 3.5083407085463523\n",
            "train loss : 14.298331402545898 , test_loss : 3.4790539622760046\n",
            "train loss : 14.157761498439925 , test_loss : 3.446563588651055\n",
            "train loss : 14.00171609670753 , test_loss : 3.4106703150089204\n",
            "train loss : 13.829427239538907 , test_loss : 3.3712211915885852\n",
            "train loss : 13.640404861534751 , test_loss : 3.328125330113243\n",
            "train loss : 13.434506221774448 , test_loss : 3.281369091348043\n",
            "train loss : 13.211996228096393 , test_loss : 3.231029801241683\n",
            "train loss : 12.973595358779262 , test_loss : 3.1772871130747253\n",
            "train loss : 12.720512965130597 , test_loss : 3.120431026912752\n",
            "train loss : 12.454463503979037 , test_loss : 3.0608653414439075\n",
            "train loss : 12.177661808536959 , test_loss : 2.9991051266807998\n",
            "train loss : 11.892792012597232 , test_loss : 2.9357669139833784\n",
            "train loss : 11.602944715173134 , test_loss : 2.8715508542787376\n",
            "train loss : 11.311519365968502 , test_loss : 2.807215059937551\n",
            "train loss : 11.02209353127342 , test_loss : 2.7435435174336558\n",
            "train loss : 10.738266547070987 , test_loss : 2.6813100612255423\n",
            "train loss : 10.463490495233494 , test_loss : 2.621241680487783\n",
            "train loss : 10.200904963919076 , test_loss : 2.5639847145062205\n",
            "train loss : 9.953192707452763 , test_loss : 2.5100772103325206\n",
            "train loss : 9.722470877422502 , test_loss : 2.459929914063198\n",
            "train loss : 9.51022749572713 , test_loss : 2.4138172003607745\n",
            "train loss : 9.317306418788524 , test_loss : 2.3718779453302625\n",
            "train loss : 9.143937599362639 , test_loss : 2.3341251654966935\n",
            "train loss : 8.989804252599386 , test_loss : 2.300462382886574\n",
            "train loss : 8.854135384240095 , test_loss : 2.27070424043401\n",
            "train loss : 8.73581125458864 , test_loss : 2.2445988821272826\n",
            "train loss : 8.6334704157144 , test_loss : 2.221849940390734\n",
            "train loss : 8.54560932840521 , test_loss : 2.202136505278468\n",
            "train loss : 8.470668510843934 , test_loss : 2.1851300493838046\n",
            "train loss : 8.407102069149376 , test_loss : 2.170507840119882\n",
            "train loss : 8.353429893306737 , test_loss : 2.1579628200322274\n",
            "train loss : 8.308273573957392 , test_loss : 2.1472102495301346\n",
            "train loss : 8.27037818792452 , test_loss : 2.1379915891846455\n",
            "train loss : 8.238622608625455 , test_loss : 2.130076173164737\n",
            "train loss : 8.212021068462597 , test_loss : 2.1232612212755657\n",
            "train loss : 8.189718486114055 , test_loss : 2.1173706835867083\n",
            "train loss : 8.17098170306546 , test_loss : 2.11225333299573\n",
            "train loss : 8.155188348869611 , test_loss : 2.107780434964028\n",
            "train loss : 8.141814638624506 , test_loss : 2.103843241529269\n",
            "train loss : 8.130423036353209 , test_loss : 2.1003504848579135\n",
            "train loss : 8.120650411043226 , test_loss : 2.0972259866443945\n",
            "train loss : 8.112197071049136 , test_loss : 2.094406453757877\n",
            "train loss : 8.104816882343593 , test_loss : 2.09183949646741\n",
            "train loss : 8.098308547774135 , test_loss : 2.089481881486285\n",
            "train loss : 8.092508037924764 , test_loss : 2.087298015954078\n",
            "train loss : 8.087282109681418 , test_loss : 2.0852586484307354\n",
            "train loss : 8.082522817683218 , test_loss : 2.083339767380214\n",
            "train loss : 8.078142909551515 , test_loss : 2.0815216751443155\n",
            "train loss : 8.074071992691081 , test_loss : 2.079788215019203\n",
            "train loss : 8.070253364468739 , test_loss : 2.0781261299759177\n",
            "train loss : 8.066641405740036 , test_loss : 2.076524533255589\n",
            "train loss : 8.063199447950982 , test_loss : 2.0749744731334547\n",
            "train loss : 8.059898035008864 , test_loss : 2.0734685763248124\n",
            "train loss : 8.056713511914376 , test_loss : 2.0720007566372938\n",
            "train loss : 8.053626882253551 , test_loss : 2.070565977461095\n",
            "train loss : 8.050622885788309 , test_loss : 2.069160058481902\n",
            "train loss : 8.047689255449184 , test_loss : 2.067779518581611\n",
            "train loss : 8.04481612001588 , test_loss : 2.066421448259296\n",
            "train loss : 8.04199552473069 , test_loss : 2.065083406072133\n",
            "train loss : 8.039221047114184 , test_loss : 2.063763334580696\n",
            "train loss : 8.036487489453544 , test_loss : 2.062459492106564\n",
            "train loss : 8.033790632913067 , test_loss : 2.0611703972938598\n",
            "train loss : 8.03112704108488 , test_loss : 2.0598947840300093\n",
            "train loss : 8.02849390314604 , test_loss : 2.058631564743911\n",
            "train loss : 8.025888908703632 , test_loss : 2.0573798004777806\n",
            "train loss : 8.023310147964663 , test_loss : 2.0561386764367358\n",
            "train loss : 8.020756032126908 , test_loss : 2.054907481970293\n",
            "train loss : 8.01822522990307 , test_loss : 2.0536855941422116\n",
            "train loss : 8.01571661690905 , test_loss : 2.052472464208656\n",
            "train loss : 8.013229235304147 , test_loss : 2.051267606456711\n",
            "train loss : 8.010762261599064 , test_loss : 2.0500705889614665\n",
            "train loss : 8.008314980968532 , test_loss : 2.048881025905523\n",
            "train loss : 8.00588676674424 , test_loss : 2.0476985711735574\n",
            "train loss : 8.003477064032488 , test_loss : 2.046522912989905\n",
            "train loss : 8.001085376616334 , test_loss : 2.045353769411659\n",
            "train loss : 7.998711256473511 , test_loss : 2.0441908845253645\n",
            "train loss : 7.996354295378205 , test_loss : 2.0430340252242862\n",
            "train loss : 7.994014118163095 , test_loss : 2.0418829784662638\n",
            "train loss : 7.9916903773048675 , test_loss : 2.0407375489307578\n",
            "train loss : 7.989382748565536 , test_loss : 2.0395975570087135\n",
            "train loss : 7.9870909274761095 , test_loss : 2.0384628370709494\n",
            "train loss : 7.984814626493055 , test_loss : 2.0373332359705287\n",
            "train loss : 7.982553572692538 , test_loss : 2.0362086117424822\n",
            "train loss : 7.980307505894826 , test_loss : 2.035088832470653\n",
            "train loss : 7.978076177133361 , test_loss : 2.033973775296631\n",
            "train loss : 7.975859347400035 , test_loss : 2.032863325549984\n",
            "train loss : 7.973656786612407 , test_loss : 2.0317573759823926\n",
            "train loss : 7.971468272759271 , test_loss : 2.03065582609124\n",
            "train loss : 7.9692935911900324 , test_loss : 2.029558581520363\n",
            "train loss : 7.967132534020014 , test_loss : 2.0284655535277576\n",
            "train loss : 7.964984899629711 , test_loss : 2.027376658511421\n",
            "train loss : 7.962850492239924 , test_loss : 2.0262918175859888\n",
            "train loss : 7.960729121548937 , test_loss : 2.0252109562038028\n",
            "train loss : 7.958620602419904 , test_loss : 2.0241340038149582\n",
            "train loss : 7.956524754609631 , test_loss : 2.023060893561716\n",
            "train loss : 7.954441402530927 , test_loss : 2.0219915620031865\n",
            "train loss : 7.952370375043128 , test_loss : 2.0209259488669016\n",
            "train loss : 7.950311505265252 , test_loss : 2.019863996824069\n",
            "train loss : 7.948264630408648 , test_loss : 2.0188056512860544\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}